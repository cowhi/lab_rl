{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from helper import get_softmax\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "global train_lib\n",
    "\n",
    "def get_importance(state=None):\n",
    "    return np.array([0.25, 0.25, 0.5])\n",
    "\n",
    "def get_combined_probs(state):\n",
    "    importance = get_importance(state)\n",
    "    summed_probs = np.array([0., 0., 0.])\n",
    "    for i in range(len(train_lib)):\n",
    "        probs = get_softmax(train_lib[i], tau=1.0)\n",
    "        summed_probs += importance[i] * probs\n",
    "    return summed_probs\n",
    "\n",
    "def get_action(state=None, tau=1.0):\n",
    "    \"\"\" Returns an action selected through softmax. \"\"\"\n",
    "    # print(self.step_current, self.tau, self.batch_size)\n",
    "    return np.random.choice(3, p=get_combined_probs(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Action:', 2)\n"
     ]
    }
   ],
   "source": [
    "train_lib = (np.array([[1, 1, 1]]),\n",
    "             np.array([[0, 1, 2]]),\n",
    "             np.array([[0, 0, 1]]))\n",
    "\n",
    "action = get_action()\n",
    "print(\"Action:\", action)\n",
    "# probabilities for each action\n",
    "#probs = defaultdict()\n",
    "#weighted_probs = defaultdict()\n",
    "\n",
    "#print(\"summed_probs = \" + str(summed_probs) + \" --> sum = \" + str(sum(summed_probs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, args, rng, log_path):\n",
    "        #_logger.info(\"Initializing Model (Type: %s)\" %\n",
    "        #             args.model)\n",
    "        self.args = args\n",
    "        self.rng = rng\n",
    "        self.log_path = log_path\n",
    "\n",
    "\n",
    "class TensorflowModel(Model):\n",
    "    def __init__(self, args, rng, session,\n",
    "                 input_shape, output_shape, log_path, scope):\n",
    "        # Call super class\n",
    "        super(TensorflowModel, self).__init__(args, rng, log_path)\n",
    "        self.scope = scope\n",
    "        self.session = session\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "class SimpleDQNModel(TensorflowModel):\n",
    "    def __init__(self, args, rng, session,\n",
    "                 input_shape, output_shape, log_path, scope):\n",
    "        # Call super class\n",
    "        super(SimpleDQNModel, self).__init__(args, rng, session,\n",
    "                                             input_shape, output_shape,\n",
    "                                             log_path, scope)\n",
    "\n",
    "        # Define a placeholder for network input\n",
    "        self.s_placeholder = tf.placeholder(\n",
    "                shape=[None] + list(self.input_shape),\n",
    "                dtype=tf.float32)\n",
    "        # Build the network\n",
    "        self.q_policy = self.build_network(scope)\n",
    "        # Define a placeholder for loss calculation\n",
    "        self.q_placeholder = tf.placeholder(shape=[None, self.output_shape],\n",
    "                                            dtype=tf.float32)\n",
    "        # Define important network parameters\n",
    "        # self.loss = tf.losses.mean_squared_error(self.q_placeholder,\n",
    "        #                                          self.q_policy)\n",
    "        self.loss = tf.losses.huber_loss(self.q_placeholder, self.q_policy)\n",
    "        # self.optimizer = tf.train.RMSPropOptimizer(self.args.alpha)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.00025)\n",
    "\n",
    "        # without gradient clipping\n",
    "        self.train_step = self.optimizer.minimize(self.loss)\n",
    "\n",
    "        # Define layer for selecting only the max value\n",
    "        self.action = tf.argmax(self.q_policy, 1)\n",
    "\n",
    "        # Define layer for a softmax output\n",
    "        # TODO check if this works or if I should use agent to do it\n",
    "        self.action_probs = tf.contrib.layers.softmax(self.q_policy)\n",
    "\n",
    "\n",
    "    def build_network(self):\n",
    "        # Create the hidden layers of the network.\n",
    "        conv1 = tf.contrib.layers.conv2d(self.s_placeholder,\n",
    "                                         num_outputs=16,\n",
    "                                         kernel_size=[8, 8],\n",
    "                                         stride=[4, 4],\n",
    "                                         scope=scope + \"/conv1\")\n",
    "        conv2 = tf.contrib.layers.conv2d(conv1,\n",
    "                                         num_outputs=32,\n",
    "                                         kernel_size=[4, 4],\n",
    "                                         stride=[2, 2],\n",
    "                                         scope=scope + \"/conv2\")\n",
    "        conv3 = tf.contrib.layers.conv2d(conv2,\n",
    "                                         num_outputs=32,\n",
    "                                         kernel_size=[3, 3],\n",
    "                                         stride=[1, 1],\n",
    "                                         scope=scope + \"/conv3\")\n",
    "        conv3_flat = tf.contrib.layers.flatten(conv3,\n",
    "                                               scope=scope + \"/conv3_flat\")\n",
    "        fc1 = tf.contrib.layers.fully_connected(conv3_flat,\n",
    "                                                num_outputs=128,\n",
    "                                                scope=scope + \"/fc1\")\n",
    "        # Create the output layer of the network\n",
    "        q = tf.contrib.layers.fully_connected(fc1,\n",
    "                                              num_outputs=self.output_shape,\n",
    "                                              activation_fn=None,\n",
    "                                              scope=scope + \"/q\")\n",
    "        return q\n",
    "\n",
    "    def train(self, state, q):\n",
    "        state = state.astype(np.float32)\n",
    "        loss_batch, _ = self.session.run([self.loss, self.train_step],\n",
    "                                         feed_dict={self.s_placeholder: state,\n",
    "                                                    self.q_placeholder: q})\n",
    "        return loss_batch\n",
    "\n",
    "    def get_qs(self, state):\n",
    "        \"\"\" Returns the Q values for all available outputs. \"\"\"\n",
    "        state = state.astype(np.float32)\n",
    "        if len(state.shape) == 3:\n",
    "            state = state.reshape([1] + list(self.input_shape))\n",
    "        return self.session.run(self.q_policy,\n",
    "                                feed_dict={self.s_placeholder: state})\n",
    "\n",
    "    def get_action_probs(self, state):\n",
    "        \"\"\" Returns a probability distribution over the possible actions. \"\"\"\n",
    "        state = state.astype(np.float32)\n",
    "        return self.session.run(self.action_probs,\n",
    "                                feed_dict={self.s_placeholder: state})\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\" Returns the index from the maximal Q value \"\"\"\n",
    "        state = state.astype(np.float32)\n",
    "        # print('Shape original', len(state.shape))\n",
    "        state = state.reshape([1] + list(self.input_shape))\n",
    "        # print('Shape altered', len(state.shape))\n",
    "        return self.session.run(self.action,\n",
    "                                feed_dict={self.s_placeholder: state})[0]\n",
    "    \n",
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(123)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = False\n",
    "config.allow_soft_placement = True\n",
    "\n",
    "# Initiate tensorflow session\n",
    "session = tf.Session(config=config)\n",
    "model_input_shape = (80, 80) + (3,)\n",
    "# Policy network\n",
    "model = SimpleDQNModel(None,\n",
    "                       rng,\n",
    "                       session,\n",
    "                       model_input_shape,\n",
    "                       3,\n",
    "                       None,\n",
    "                       \"policy\")\n",
    "\n",
    "# target network\n",
    "target_model = SimpleDQNModel(None,\n",
    "                              rng,\n",
    "                              session,\n",
    "                              model_input_shape,\n",
    "                              3,\n",
    "                              None,\n",
    "                              \"target\")\n",
    "init = tf.global_variables_initializer()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01797529 -0.17344828 -0.13216183]]\n",
      "[[-0.00854884 -0.00646538 -0.12156008]]\n",
      "--------------- Copying -----------------\n",
      "[[-0.01797529 -0.17344828 -0.13216183]]\n",
      "[[-0.01797529 -0.17344828 -0.13216183]]\n"
     ]
    }
   ],
   "source": [
    "s = np.ones((1,) + model_input_shape, dtype=np.uint8)\n",
    "\n",
    "print(model.get_qs(s))\n",
    "print(target_model.get_qs(s))\n",
    "print(\"--------------- Copying -----------------\")\n",
    "copy_model_parameters(session, model, target_model)\n",
    "print(model.get_qs(s))\n",
    "print(target_model.get_qs(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " var   0: (8, 8, 3, 16)   policy/conv1/weights:0\n",
      " var   1: (16,)           policy/conv1/biases:0\n",
      " var   2: (4, 4, 16, 32)  policy/conv2/weights:0\n",
      " var   3: (32,)           policy/conv2/biases:0\n",
      " var   4: (3, 3, 32, 32)  policy/conv3/weights:0\n",
      " var   5: (32,)           policy/conv3/biases:0\n",
      " var   6: (3200, 128)     policy/fc1/weights:0\n",
      " var   7: (128,)          policy/fc1/biases:0\n",
      " var   8: (128, 3)        policy/q/weights:0\n",
      " var   9: (3,)            policy/q/biases:0\n",
      " var  10: (8, 8, 3, 16)   target/conv1/weights:0\n",
      " var  11: (16,)           target/conv1/biases:0\n",
      " var  12: (4, 4, 16, 32)  target/conv2/weights:0\n",
      " var  13: (32,)           target/conv2/biases:0\n",
      " var  14: (3, 3, 32, 32)  target/conv3/weights:0\n",
      " var  15: (32,)           target/conv3/biases:0\n",
      " var  16: (3200, 128)     target/fc1/weights:0\n",
      " var  17: (128,)          target/fc1/biases:0\n",
      " var  18: (128, 3)        target/q/weights:0\n",
      " var  19: (3,)            target/q/biases:0\n"
     ]
    }
   ],
   "source": [
    "tvar = tf.trainable_variables()\n",
    "\n",
    "for idx, v in enumerate(tvar):\n",
    "    print(\" var {:3}: {:15} {}\".format(idx, str(v.get_shape()), v.name))\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
